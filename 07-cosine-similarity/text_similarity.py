# -*- coding: utf-8 -*-
"""cosine-similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jIA4OQQl593mXeL_SVT8bVdDKRta_mrs

# Lab Problem:

In class this week we learned about cosine similarity as a way of measuring
similarity in text documents. Today, we’re focused on the similarity of fictional presidents instead of real ones, but working through the lab will still be helpful as you get deeper into HW3.


The text files for this lab are speeches given by President Bartlet (from the West Wing) and President Shepard (from the American President), and we’re going
to see how similar they are,
using (1) visualization, and (2) cosine similarity.

Both characters are written by Aaron Sorkin, but the speeches are in totally
different contexts, 

so it’s kind of hard to predict what we’ll find.
You may use the code we did together in class to compute the cosine similarity
of two texts based on the dot product of two vectors.

Here’s the approach today:

• Read in each text file as a string.

• Visualize both Presidents’ speeches using the wordcloud library (also discussed in Tuesday’s lecture).

• Find the most common words used by each President. You might find it
convenient to use the Counter module to do this. Try creating vectors
using the unique words among the most frequent k words of each speech,
where k=10-20 words (your choice).


• Use the cosine similarity function provided in class to measure the
similarity of the two speeches. Is it what you expected? Would you have
guessed that these characters were written by the same person?

• As an optional enhancement, try cleaning up the text to remove small
words and punctuation. This might produce a more accurate result.
"""

# Install python packages
#!pip install matplotlib pandas wordcloud

"""# Word cloud"""

# import python packages
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

import math
import re
from collections import Counter

# Read txt file and store it as string

with open('bartlet.txt', 'r') as file:
    bartlet = file.read().replace('\n', '')

with open('shepard.txt', 'r') as file:
    shepard = file.read().replace('\n', '')

# Generate word cloud for bartlet

bartlet_word_cloud = WordCloud(width= 3000, height = 2000,
                              random_state=1,
                              colormap='Pastel1',
                              collocations=False, 
                              stopwords = STOPWORDS).generate(bartlet)

plt.figure(figsize=(20, 10))
plt.axis('off')
plt.savefig("bartlet.png")
plt.imshow(bartlet_word_cloud)

shepard_word_cloud = WordCloud(width= 3000, height = 2000,
                              random_state=1,
                              colormap='Pastel1',
                              collocations=False, 
                              stopwords = STOPWORDS).generate(shepard)

plt.figure(figsize=(20, 10))
plt.axis("off")
plt.savefig("shepared.png")
plt.imshow(shepard_word_cloud)

"""# Cosine similarity

One of the technique to find similarity between two text documents is called Cosine similarity. 

Similar in terms of what? That's the main question when chooses to find the similarity? Based on word count on both documents or meaning of both documents?

If we took above two features ie word count and how similar to each other and place it on the x and y axis as a vector. 

The difference between two vectors forms an angle.

This angle will helps to identify how similar or different are the documents.

0-deg = same
90-deg = opposite to each other

Knowing the angle will tell you how similar the text documents are, but it's better to have placed the value between 0 to 1. that's where the cosine similarity coming into picture.

**compute cosine similarity of A to B: (A dot B)/{||A||*||B||)**

A & B are vector representation of two text document.
"""

# Stop words
stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]

# Regular expression Get only words.
WORD = re.compile(r"\w+")

# Calculate cosine similarity
# It will take argument which is vector representation of both document.

def get_cosine(vec1, vec2):

    intersection = set(vec1.keys()) & set(vec2.keys())

    numerator = sum([vec1[x] * vec2[x] for x in intersection])

    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])
    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])

    denominator = math.sqrt(sum1) * math.sqrt(sum2)

    if not denominator:
        return 0.0
    else:
        return float(numerator) / denominator

# Convert text into vector and removing stop words
def text_to_vector(text):
    words = WORD.findall(text) # applying regular expression and get only text data
    word_list = [] # Empty list to store data
    for word in words: 
        if word not in stop_words: # Check if the word is stop word or not
            word_list.append(word)
    
    return Counter(word_list) # Return counter object

# Convert text into vector without removing stop words
def text_to_vector_with_sw(text):
    words = WORD.findall(text)
    return Counter(words)

"""There are many ways available to convert text from vectors. Here used simple count Vectorizer which take unique words and fit them by giving index the go thorough the text document and update the count of unique words when present.

Stop words are set of word commonly used words in any language. If we remove the stop words, we can able to focus on the important words.
"""

# By removing stop word
A = text_to_vector(bartlet) # convert text into vector
B = text_to_vector(shepard)
cosine = get_cosine(A, B)
print("Cosine Similarity:", cosine)

print(A)

print(B)

# Without removing stop word
vector1 = text_to_vector_with_sw(bartlet)
vector2 = text_to_vector_with_sw(shepard)
cosine_sw = get_cosine(vector1, vector2)
print("Cosine Similarity:", cosine_sw)

print(vector1)

print(vector2)

"""* Around 30% similarity when removed the stop words from the document.
* Around 63% similarity between documents when used as it is.

# Most common words used by each President
"""

# By removing stop word
barlet_words = text_to_vector(bartlet)

# We are saving result into counter object so we can use most_common to retrive K words
barlet_words.most_common(10)

# By removing stop word
shepard_words = text_to_vector(shepard)

shepard_words.most_common(10)

# With stop word
barlet_words_wsw = text_to_vector_with_sw(bartlet)
barlet_words_wsw.most_common(20)

# With stop word
shepard_words_wsw = text_to_vector_with_sw(shepard)
shepard_words_wsw.most_common(20)

